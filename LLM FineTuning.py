# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Tw76phqBbUebgH76H4wpGEeqlDVxUbK
"""

!pip install transformers datasets

from datasets import load_dataset
dataset = load_dataset("imdb",split="train[:1%]")

print(dataset[0])

dataset.shape

def preprocess(batch):
  batch["text"]=[text.replace("\n",' ') for text in batch["text"]]
  return batch
dataset = dataset.map(preprocess,batched=True)

from transformers import AutoTokenizer, AutoModelForCausalLM
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
  tokenized=tokenizer(examples["text"],padding="max_length",truncation=True,max_length=128)
  tokenized["labels"]=tokenized["input_ids"].copy()
  return tokenized

tokenized_data = dataset.map(tokenize_function, batched=True)

tokenized_data

from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=20,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=1

)

Train_data = tokenized_data.shuffle().select(range(int(len(tokenized_data)*0.8)))
val_data = tokenized_data.shuffle().select(range(int(len(tokenized_data)*0.8),len(tokenized_data)))

from transformers import Trainer
trainer = Trainer(model=model,args=training_args,train_dataset=Train_data,eval_dataset=val_data)

trainer.train()

model.save_pretrained("./results")
tokenizer.save_pretrained("./results")

prompt = "The script"
inputs = tokenizer(prompt, return_tensors="pt")
output = model.generate(inputs['input_ids'], max_length=15)
print(tokenizer.decode(output[0], skip_special_tokens=True))